# RFP Knowledge Base: Embedding Strategy & Process

## ğŸ“Š Analysis of RFP Files

### File Structure Discovered

```
resources/RFP_Hackathon/
â”œâ”€â”€ ASM/                                    # Semiconductor - Talent Intelligence
â”œâ”€â”€ ARM - Job Listings/                    # Job Postings Data
â”œâ”€â”€ ARM - Training Provider Database/      # Training Provider RFP
â”œâ”€â”€ Atlassian/                             # Market Data RFP (SaaS)
â”œâ”€â”€ Denso/                                 # Skills Architecture
â”œâ”€â”€ GMR Group/                             # Learning Tech Solution
â”œâ”€â”€ Liberty Mutual - Workforce Analytics/  # Labor Market Analytics
â”œâ”€â”€ TE - Skills Taxonomy/                  # Skills Taxonomy
â”œâ”€â”€ Tennessee/                             # Labor Market Analysis
â””â”€â”€ Lockheed/                              # Labor Market RFI

Total: ~91 documents (PDF, DOCX, XLSX, TXT)
```

### Document Categories

**Three Main Types:**
1. **Received/** - Client RFP documents (Questions, Requirements)
2. **Final/Sent/** - Draup's responses (Answers, Proposals)
3. **Attachments/** - Supporting documents (Policies, Reports, Data Dictionaries)

---

## ğŸ¯ Embedding Strategy

### First Principles: What Are We Really Embedding?

**Mental Model**: Each RFP is a *knowledge triplet*:
- **Question** (What the client asked)
- **Answer** (How Draup responded)
- **Context** (Client industry, requirements, outcome)

### Key Insight:
Traditional embedding approaches treat documents as isolated text. But RFPs are **paired conversations**. We need to:
1. Link questions to answers
2. Preserve client context
3. Enable "similar client" searches
4. Support "similar question" retrieval

---

## ğŸ“ Chunking Strategy

### Challenge: RFPs Have Different Structures

**Three Chunking Approaches:**

### 1. **Structural Chunking** (Best for RFPs)

```
Document â†’ Sections â†’ Questions â†’ Answers
```

**Example from ASM RFP:**
```
Section: "Our Request"
â”œâ”€â”€ Q1: Provide demo of Talent Intelligence platform
â”œâ”€â”€ Q2: Focus on hiring analysis functionality
â”œâ”€â”€ Q3: Focus on labor market analysis
â””â”€â”€ ... (13 questions total)
```

**Chunk Size**: Each question-answer pair = 1 chunk
- **Pros**: Preserves semantic meaning, enables QA retrieval
- **Cons**: Some chunks may be large (3000+ chars)

### 2. **Semantic Chunking** (For long documents)

```python
# For documents >2000 tokens
- Split by semantic boundaries (paragraphs, sections)
- Max chunk size: 512 tokens
- Overlap: 50 tokens (to preserve context at boundaries)
```

**When to use**: Long proposal documents, case studies

### 3. **Hybrid Chunking** (Recommended)

```
1. Extract structured Q&A pairs â†’ Store as linked chunks
2. For narrative sections â†’ Use semantic chunking
3. For tables/data â†’ Store as structured metadata
```

---

## ğŸ§  Embedding Techniques

### Current Approach: `sentence-transformers/all-MiniLM-L6-v2`

**Pros:**
- Fast (6 layers, 384 dimensions)
- Good for general semantic search
- Works offline

**Cons:**
- Limited domain knowledge (not trained on RFPs)
- 512 token limit

### Recommended: **Multi-Model Embedding Strategy**

#### **Option 1: Hybrid Embeddings** (Best for MVP)

```python
# Combine multiple embedding models
primary_model = "all-MiniLM-L6-v2"        # Fast, general
domain_model = "all-mpnet-base-v2"       # Better quality, slower

# For each chunk:
# 1. Generate both embeddings
# 2. Store both in vector DB
# 3. At search time, use weighted combination
```

**Why**: Different models capture different aspects
- MiniLM: Fast syntactic similarity
- MPNet: Deeper semantic understanding

#### **Option 2: Fine-Tuned Embeddings** (For Production)

```python
# Fine-tune on RFP-specific data
base_model = "sentence-transformers/all-mpnet-base-v2"

# Training data: Question-Answer pairs from past RFPs
# Loss function: ContrastiveLoss (similar Q&A close, dissimilar apart)
```

**Training Set:**
- Positive pairs: (Question, Its Answer)
- Negative pairs: (Question, Unrelated Answer)

#### **Option 3: OpenAI Embeddings** (Highest Quality)

```python
# Use OpenAI's text-embedding-3-large
model = "text-embedding-3-large"
dimensions = 3072  # vs 384 for MiniLM

# Pros: Best quality, handles long context
# Cons: API cost, requires internet
```

**Cost Analysis:**
- 91 documents Ã— ~5000 tokens avg = 455K tokens
- Cost: ~$0.06 total (one-time)
- **Recommendation**: Use for production, worth the quality

---

## ğŸ—‚ï¸ Metadata Extraction Strategy

### Critical Metadata Fields

```python
{
    # Document Identifiers
    "doc_id": "ASM-RFP-2025-Talent-Intelligence",
    "doc_type": "rfp_received",  # rfp_received, rfp_response, attachment
    "file_path": "resources/RFP_Hackathon/ASM/Received/...",

    # Client Context
    "client_name": "ASM",
    "industry": "Semiconductor",
    "company_size": "4500+",
    "geographic_focus": ["Japan", "Korea", "Taiwan", "China", "Europe", "US"],

    # RFP Metadata
    "rfp_date": "2025-05-20",
    "deadline": "2025-05-30",
    "decision_date": "2025-06-30",

    # Content Classification
    "category": "talent_intelligence",  # technical, legal, pricing, case_study
    "subcategories": [
        "hiring_analysis",
        "labor_market_analysis",
        "peer_analysis",
        "compensation_analysis"
    ],

    # Requirements
    "key_requirements": [
        "Global talent data coverage",
        "Asia-Pacific data (Japan, Korea, Taiwan, China)",
        "Semiconductor industry focus",
        "Peer benchmarking",
        "Strategic reporting"
    ],

    # Outcome Tracking
    "win_status": True,  # True, False, None (pending)
    "contract_value": 150000,  # if known
    "renewal_status": "active",  # active, churned, pending

    # Linked Documents
    "related_docs": [
        "ASM-Response-2025-Final.pdf",
        "ASM-Demo-Presentation.pdf"
    ],

    # Searchability
    "keywords": [
        "talent intelligence",
        "semiconductor",
        "hiring analysis",
        "labor market",
        "Asia Pacific"
    ],

    # Usage Tracking
    "times_retrieved": 0,
    "last_used": None,
    "quality_score": 0.95  # based on win rate, recency
}
```

### Metadata Extraction Process

```python
def extract_metadata(file_path: str, doc_text: str) -> dict:
    """
    Extract metadata using LLM + heuristics
    """
    # 1. Extract from file path
    path_parts = file_path.split("/")
    client_name = path_parts[2]  # "ASM"
    doc_type = "received" if "Received" in file_path else "final"

    # 2. Extract from document content (LLM)
    prompt = f"""
    Extract structured metadata from this RFP document:

    {doc_text[:2000]}

    Extract:
    - Client industry
    - Key requirements (list)
    - Geographic focus
    - Categories (technical, legal, pricing, etc.)
    - Timeline/dates mentioned
    - Success criteria

    Return as JSON.
    """

    metadata = llm.generate_structured(prompt)

    # 3. Enhance with domain knowledge
    metadata["client_name"] = client_name
    metadata["doc_type"] = doc_type
    metadata["file_path"] = file_path

    return metadata
```

---

## ğŸ”„ Complete Knowledge Ingestion Process

### **Step-by-Step Pipeline**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                1. DOCUMENT DISCOVERY                         â”‚
â”‚  â€¢ Scan resources/RFP_Hackathon/ recursively                â”‚
â”‚  â€¢ Filter by file type (PDF, DOCX, XLSX, TXT)               â”‚
â”‚  â€¢ Group by client folders                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                2. DOCUMENT EXTRACTION                        â”‚
â”‚  â€¢ PDF â†’ PyPDF2 / pdfplumber                                â”‚
â”‚  â€¢ DOCX â†’ python-docx                                       â”‚
â”‚  â€¢ XLSX â†’ openpyxl (convert tables to text)                 â”‚
â”‚  â€¢ TXT â†’ direct read                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                3. STRUCTURE DETECTION                        â”‚
â”‚  â€¢ Detect Q&A pairs (regex patterns)                        â”‚
â”‚  â€¢ Identify sections (headers, numbering)                   â”‚
â”‚  â€¢ Extract tables and structured data                       â”‚
â”‚  â€¢ Link Request â†’ Response documents                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                4. METADATA EXTRACTION                        â”‚
â”‚  â€¢ LLM-based extraction (client, industry, reqs)            â”‚
â”‚  â€¢ Path-based extraction (client name, doc type)            â”‚
â”‚  â€¢ Date extraction (NER for timeline)                       â”‚
â”‚  â€¢ Keyword extraction (TF-IDF + LLM)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                5. CHUNKING                                   â”‚
â”‚  â€¢ Structural chunks: Q&A pairs                             â”‚
â”‚  â€¢ Semantic chunks: Long narratives (512 tokens max)        â”‚
â”‚  â€¢ Preserve overlap (50 tokens)                             â”‚
â”‚  â€¢ Attach metadata to each chunk                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                6. EMBEDDING GENERATION                       â”‚
â”‚  â€¢ Generate embeddings (sentence-transformers)              â”‚
â”‚  â€¢ Option: Multi-model (MiniLM + MPNet)                     â”‚
â”‚  â€¢ Option: OpenAI embeddings for critical docs              â”‚
â”‚  â€¢ Store: [embedding_vector, chunk_text, metadata]          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                7. VECTOR STORE INGESTION                     â”‚
â”‚  â€¢ Add to FAISS index                                       â”‚
â”‚  â€¢ Store metadata in parallel array                         â”‚
â”‚  â€¢ Create client-specific indices (optional)                â”‚
â”‚  â€¢ Build reverse index (metadata â†’ chunks)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                8. POST-PROCESSING                            â”‚
â”‚  â€¢ Link Q&A pairs (bidirectional)                           â”‚
â”‚  â€¢ Create summary embeddings (per document)                 â”‚
â”‚  â€¢ Build client similarity graph                            â”‚
â”‚  â€¢ Index for faceted search (industry, category, etc.)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                9. VALIDATION & QA                            â”‚
â”‚  â€¢ Test retrieval: "semiconductor talent intelligence"      â”‚
â”‚  â€¢ Verify metadata accuracy (spot check)                    â”‚
â”‚  â€¢ Measure embedding quality (cosine similarity tests)      â”‚
â”‚  â€¢ Log statistics (chunks per doc, avg chunk size)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” Advanced Retrieval Strategies

### 1. **Hybrid Search** (Recommended)

```python
def hybrid_search(query: str, filters: dict = None, top_k: int = 5):
    """
    Combine semantic search + metadata filtering
    """
    # Step 1: Semantic search (cast wide net)
    semantic_results = vector_store.search(query, top_k=top_k*3)

    # Step 2: Apply metadata filters
    filtered = [
        r for r in semantic_results
        if matches_filters(r.metadata, filters)
    ]

    # Step 3: Re-rank by:
    # - Semantic similarity (40%)
    # - Industry match (30%)
    # - Win rate (20%)
    # - Recency (10%)
    reranked = rerank(filtered, weights=[0.4, 0.3, 0.2, 0.1])

    return reranked[:top_k]
```

### 2. **Question-Answer Linking**

```python
# When a query matches a question chunk:
# 1. Return the question chunk
# 2. ALSO return its linked answer chunk
# 3. Include context from related questions

def retrieve_with_context(query: str):
    """
    Retrieve Q&A pairs with context
    """
    # Find matching question
    question_chunk = vector_store.search(query, filters={"type": "question"}, top_k=1)[0]

    # Get linked answer
    answer_chunk = get_linked_chunk(question_chunk.metadata["answer_id"])

    # Get related questions from same RFP
    related_qs = vector_store.search(
        query,
        filters={"rfp_id": question_chunk.metadata["rfp_id"]},
        top_k=3
    )

    return {
        "question": question_chunk,
        "answer": answer_chunk,
        "related_questions": related_qs
    }
```

### 3. **Client Similarity Search**

```python
# Find similar clients to personalize responses
def find_similar_clients(target_client: str, top_k: int = 3):
    """
    Find clients with similar characteristics
    """
    target_metadata = get_client_metadata(target_client)

    similar_clients = []
    for client in all_clients:
        similarity = calculate_client_similarity(
            target_metadata,
            get_client_metadata(client),
            weights={
                "industry": 0.4,
                "company_size": 0.2,
                "geographic_focus": 0.2,
                "categories": 0.2
            }
        )
        similar_clients.append((client, similarity))

    return sorted(similar_clients, key=lambda x: x[1], reverse=True)[:top_k]
```

---

## ğŸ“ˆ Quality Metrics

### Embedding Quality Tests

```python
# Test 1: Semantic Coherence
query = "semiconductor talent intelligence platform"
results = search(query)
# Expected: ASM RFP should be top result

# Test 2: Cross-Client Similarity
query = "skills taxonomy implementation"
results = search(query)
# Expected: TE (Skills Taxonomy) and GMR (Skills Architecture) should rank high

# Test 3: Geographic Filtering
query = "Asia Pacific talent data"
results = search(query, filters={"geographic_focus": "Asia"})
# Expected: ASM (Japan/Korea/Taiwan/China focus) should be top

# Test 4: Q&A Pair Linking
query = "Provide demo of your platform"
qa_pair = retrieve_with_context(query)
# Expected: Question from ASM RFP + linked Answer from Draup response
```

---

## ğŸš€ Implementation Priorities

### **Phase 1: MVP (Week 1)**
1. âœ… Basic document extraction (PDF, DOCX, TXT)
2. âœ… Simple chunking (paragraph-based, 512 tokens)
3. âœ… Single embedding model (MiniLM)
4. âœ… FAISS vector store
5. âœ… Basic metadata (client, doc_type, file_path)

### **Phase 2: Enhanced (Week 2)**
1. ğŸ”„ Structural Q&A detection
2. ğŸ”„ LLM-based metadata extraction
3. ğŸ”„ Hybrid search (semantic + filters)
4. ğŸ”„ Q&A linking
5. ğŸ”„ Client similarity scoring

### **Phase 3: Production (Week 3-4)**
1. â³ Fine-tuned embeddings (RFP-specific)
2. â³ Multi-model embedding
3. â³ Advanced re-ranking
4. â³ Automatic win/loss outcome tracking
5. â³ Analytics dashboard

---

## ğŸ’¡ Recommendations

### **For Hackathon (Next 48 hours):**

1. **Use OpenAI Embeddings** (`text-embedding-3-small`)
   - Cost: ~$0.02 for entire dataset
   - Quality: 10x better than MiniLM
   - Time saved: No need for fine-tuning

2. **Focus on Metadata Extraction**
   - Client name, industry, categories
   - This is 70% of retrieval quality
   - Use LLM to extract in batch

3. **Implement Q&A Linking**
   - Manually map Received/ â†’ Final/ pairs
   - Store as linked chunks
   - Huge impact on response quality

4. **Build Simple Web UI**
   - Search box â†’ Results with metadata
   - Filter by client, industry, category
   - Show Q&A pairs side-by-side

### **Key Success Metrics:**

1. **Retrieval Accuracy**: 90%+ relevant results in top-3
2. **Speed**: <500ms for search queries
3. **Coverage**: All 91 documents embedded
4. **Metadata Quality**: 95%+ fields populated correctly

---

## ğŸ“ Next Steps

See implementation in:
- `services/embedding_service.py` - Enhanced embedding logic
- `services/metadata_extractor.py` - LLM-based metadata extraction
- `scripts/ingest_rfp_knowledge.py` - Batch ingestion pipeline
- `scripts/validate_embeddings.py` - Quality validation tests

